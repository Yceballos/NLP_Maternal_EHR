#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jun 30 18:23:55 2022

@author: yasmin
"""

import os
import pickle
import csv
import logging
import sys
from transformers import AutoTokenizer
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import BertTokenizer, BertConfig, BertForTokenClassification
from transformers import AdamW, get_linear_schedule_with_warmup
from sklearn.metrics import precision_score, recall_score, f1_score
from nervaluate import Evaluator
import copy


class PretrainDataset(Dataset):

    '''
    Pytorch dataset class for data hc (100 hce)
    
    parameters:
      - pickle_path: str
        path to pickle file containing pretraining dataset, generated by 
        data_processing/data.pickle 
        
      - tokenizer_path: str
        path to BERT vocabulary file
      - pretrain_dataset: str (default: 'medmentions')
        pretraining dataset, can be 'medmentions' or 'semmed'
       
    methods:
      - process_text(sentences,labels,max_len=128)
        convert raw text and labels into BERT model inputs
      - convert_bert_outputs(ids,labels)
        converts output from BERT model for a single sample to NER labels for nervaluate tool
    '''
    
    def __init__(self,pickle_path,tokenizer_path):

        # Load pre-trained model tokenizer (vocabulary)
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        
        self.idx2label = {0:'O',
                          1:'B-Procedimiento',
                          2:'B-Anatomía',
                          3:'B-Signo o síntoma',
                          4:'B-Atributo',
                          5:'B-Negación',
                          6:'B-Problema clínico',
                          7:'B-Sustancia', 
                         } ###8 clases 
        
        
        self.label2idx = {v: k for k, v in self.idx2label.items()}
        
        
        # self.idx2label = {0:'O',
        #                   1:'B-Procedimiento',
        #                   2:'B-Anatomía',
        #                   3:'B-Signo o síntoma',
        #                   4:'B-Atributo',
        #                   5:'B-Negación',
        #                   6:'B-Problema clínico',
        #                   7:'B-Sustancia', 
        #                   8:'I-Procedimiento',
        #                   9:'I-Anatomía',
        #                   10:'I-Signo o síntoma',
        #                   11:'I-Atributo',
        #                   12:'I-Negación',
        #                   13:'I-Problema clínico',
        #                   14:'I-Sustancia'} ###15
        
        
  
            
        
        self.num_classes = 9
        
        #load data
        with open(pickle_path,'rb') as f:
            data = pickle.load(f)
                
        sentences = []
        labels = []

        for j in data:
            for k, (l,v) in enumerate(data[j].items()):
                #print(v)
                sentences.append(v['sentences'])
                labels.append(v['labels'])
            
            
        ##Change some errors labeling by hand
        ###Correciones de etiquetas: 
        
        labels[482][12]='B-Atributo'
        labels[482][13]='B-Atributo'
        labels[482][14]='B-Atributo'
        labels[482][15]='B-Atributo'
        labels[482][16]='B-Atributo'
        
        labels[506][34]='B-Procedimiento'
        labels[506][35]='B-Procedimiento'
        
        labels[904][21]='B-Procedimiento'
        labels[904][110]='B-Procedimiento'
        
        labels[905][24]='B-Procedimiento'
        
        labels[998][150]='B-Atributo'
        
        labels[1005][88]='O'
        
        labels[1088][16]='O'
        labels[1088][17]='O'
        
        labels[1117][94]='O'
        labels[1117][95]='O'
        
        labels[1321][9]='O'
        labels[1321][10]='O'
        labels[1321][11]='O'
        labels[1321][12]='O'
        labels[1321][13]='O'
        
        labels[1332][4]='O'
        
        labels[1358][8]='O'
        
        
        labels2=copy.deepcopy(labels) 

        for i in range(len(labels2)):
            
            for j, etiqueta in enumerate(labels2[i]):
                
                if etiqueta=="I-Atributo":
                    labels2[i][j]= "B-Atributo"
                    
                if etiqueta=="I-Sustancia":
                    labels2[i][j]="B-Sustancia"
                    
                if etiqueta=="I-Procedimiento":
                    labels2[i][j]="B-Procedimiento"
                    
                if etiqueta=="I-Signo o síntoma":
                    labels2[i][j]="B-Signo o síntoma"
                    
                if etiqueta=="I-Negación":
                    labels2[i][j]="B-Negación"
                    
                if etiqueta=="I-Anatomía":
                    labels2[i][j]="B-Anatomía"
                    
                if etiqueta=="I-Problema clínico":
                    labels2[i][j]="B-Problema clínico"
                    
                    
                    
        
                    
        #clean text
        self.tokens,self.segids,self.masks,self.labels2,self.doc_lens =\
                        self.process_text(sentences,labels2)

    def process_text(self,sentences,labels2,max_len=128):
    
        '''
        convert raw text and labels into BERT model inputs
        
        parameters:
          - sentences: list[list[str]]
            list of raw text from NER .tsv files, each str is one word
          - labels: list[list[str]]
            list of raw labels from NER .tsv files, each str is one label (B, I, O)
          - tokenizer_path: str
            path to BERT vocabulary file
            
        outputs:
          - tokens: list[list[int]]
            input id tokens for BERT model
          - seg_ids: list[list[int]]
            segment ids for BERT model
          - masks: list[list[int]]
            mask ids for BERT model
          - ner_labels: list[list[int]]
            ner labels for BERT model
          - doc_lens: list[int]
            count of non-padding tokens for each sample
        '''
        
        #outputs per sentence
        
        tokens = []
        seg_ids = []
        masks = []
        ner_labels = []
        doc_lens = []

        for i,(sent,label) in enumerate(zip(sentences,labels2)):

            #tokenize each sentence, matching labels to tokens
            text_segment = []
            converted_labels = []
            mask = []
            for j,(word,lab) in enumerate(zip(sent,label)):
                tokenized_text = self.tokenizer.tokenize(word.lower())
                text_segment.extend(tokenized_text)
                l = len(tokenized_text)  
                
                label_id = self.label2idx[lab]
                #print(i,j)
                converted_labels.extend([label_id]*l)
                if l > 0:
                    mask.extend([1]+[0]*(l-1))
                
            #fit into max length
            text_segment = text_segment[:(max_len-2)]
            converted_labels = converted_labels[:(max_len-2)]
            mask = mask[:(max_len-2)]
            
            #add special tokens
            text_segment = ['[CLS]'] + text_segment + ['[SEP]']
            converted_labels = [0] + converted_labels + [0]
            mask = [1] + mask + [1]
            
            #convert into ids
            indexed_tokens = self.tokenizer.convert_tokens_to_ids(text_segment)
            
            #pad everything to max len
            l = len(indexed_tokens) if len(indexed_tokens) < max_len else max_len
            l_pad = max_len - l
            indexed_tokens += [0] * l_pad
            converted_labels += [0] * l_pad
            mask += [0] * l_pad
            
            #append final output
            tokens.append(indexed_tokens)
            ner_labels.append(converted_labels)
            seg_ids.append([0 for i in indexed_tokens])
            masks.append(mask)
            doc_lens.append(l)
            
            sys.stdout.write("processed %i lines      \r" % i)
            sys.stdout.flush()
            
        print()
        return tokens,seg_ids,masks,ner_labels,doc_lens 
    
    def __len__(self):
        return len(self.tokens)

    def __getitem__(self,idx):
        sample = {'tokens': torch.tensor(self.tokens[idx],dtype=torch.long),
                  'masks': torch.tensor(self.masks[idx],dtype=torch.long),
                  'seg_ids': torch.tensor(self.segids[idx],dtype=torch.long),
                  'labels': torch.tensor(self.labels2[idx],dtype=torch.long),
                  'doc_lens': self.doc_lens[idx]}

        return sample
        
    def convert_bert_outputs(self,ids,labels2):
    
        '''
        converts output from BERT model for a single sample to NER labels for nervaluate tool
        
        parameters:
          - ids: list[int]
            input id tokens for BERT model for a single sample
          - labels: list[int]
            BERT model predicted labels for a single sample
            
        outputs:
          - text: list[str]
            raw text of original input
          - final labels: list[str]
            NER labels (B-ENT,I-ENT,O-ENT) for nervaluate tool
        '''

        cleaned_labels = []
        text = self.tokenizer.convert_ids_to_tokens(ids)
        for i,(t,l) in enumerate(zip(text,labels2)):
            if t[:2] != '##':
                cleaned_labels.append(self.idx2label[l])
            
        final_labels = []
        prev_entry = 'O'
        for l in cleaned_labels:
            cur_entry = l
            if prev_entry == 'O' and cur_entry == 'I-ENT':
                cur_entry = 'B-ENT'
            final_labels.append(cur_entry)
            prev_entry = cur_entry
            
        text = self.tokenizer.convert_tokens_to_string(text)
        text = text.split()
        
        return text[1:-1],final_labels[1:-1]